# Any passwords and credentials in this file should be encrypted

# All default properties for stack "dev" instance "${org.sagebionetworks.developer}" belong in this file

#---- Endpoint configuration
# Crowd url must include protocol (http or https) and port (e.g. 8443)
org.sagebionetworks.crowd.endpoint=https://dev-crowd.sagebase.org:8443
org.sagebionetworks.authenticationservice.privateendpoint=${org.sagebionetworks.authenticationservice.privateendpoint}
org.sagebionetworks.authenticationservice.publicendpoint=${org.sagebionetworks.authenticationservice.publicendpoint}
org.sagebionetworks.repositoryservice.endpoint=${org.sagebionetworks.repositoryservice.endpoint}
org.sagebionetworks.fileservice.endpoint=${org.sagebionetworks.fileservice.endpoint}
org.sagebionetworks.searchservice.endpoint=${org.sagebionetworks.searchservice.endpoint}
org.sagebionetworks.portal.endpoint=${org.sagebionetworks.portal.endpoint}

#--- The main MySQL database connection.
org.sagebionetworks.repository.database.connection.url=jdbc:mysql://localhost/dev${org.sagebionetworks.developer}
org.sagebionetworks.repository.database.username=dev${org.sagebionetworks.developer}
org.sagebionetworks.repository.database.password=tK15UbS9itkodIYeyDQpQA==

#--- The database information used to create ids.
org.sagebionetworks.id.generator.database.connection.url=jdbc:mysql://localhost/dev${org.sagebionetworks.developer}
org.sagebionetworks.id.generator.database.username=dev${org.sagebionetworks.developer}
org.sagebionetworks.id.generator.database.password=tK15UbS9itkodIYeyDQpQA==

#--- Crowd Passwords
org.sagebionetworks.crowdApplicationKey=tK15UbS9itnJJ9+MVY+GHro0ZItb9Z/I
org.sagebionetworks.mailPW=KvmD+SYn9DJyz494VE5nwDSjwNf/rW6pKHSGHsg0KUA=

#---- S3 configuration

# This is the bucket for Synapse data
org.sagebionetworks.s3.readAccessExpiryHours=24
org.sagebionetworks.s3.writeAccessExpiryHours=24
# This is for attachment URLs that expire in 10 seconds.
org.sagebionetworks.s3.readAccessExpirySeconds=20
org.sagebionetworks.s3.bucket=devdata.sagebase.org

# This is the shared bucket for all backup files.
org.sagebionetworks.shared.s3.backup.bucket=shared.backups.sagebase.org

# This is the bucket for workflow-related files such as configuration or search document files.
# Each workflow should store stuff under its own workflow name prefix so that we can configure 
# permissions not only on the bucket but also the S3 object prefix.
org.sagebionetworks.s3.bucket.workflow=devworkflow.sagebase.org

#--- encrypted values (passwords, credentials):
#
# to generate an encrypted property value:
# mvn exec:java -Dexec.mainClass="org.sagebionetworks.StringEncrypter" -Dexec.args="MySecretPassword common-encryption-key-for-stack"
# returns 
# MySecretPassword -> PZnry4SROFQVIq1up8DIt8uxV8TkD0Ve
# The actual encryption key is saved in work/platform/PasswordsAndCredentials/platformPropertyEncryptionKey.txt

# application API Key, used for Crowd REST API  NOTE:  This is for the dev instance of Crowd
# 8-8-2011 commented out the following, since it's defined in another .properties file
# org.sagebionetworks.crowdApplicationKey=YJxbfSsxenhUbwz9IVQaOA==

org.sagebionetworks.id.generator.database.driver=com.mysql.jdbc.Driver
org.sagebionetworks.repository.databaes.driver=com.mysql.jdbc.Driver

# The repository database connection pool properties.
org.sagebionetworks.pool.connection.validate=true
# This is the SQL that will be used to determine if a connection is alive.
org.sagebionetworks.pool.connection.validate.sql=SELECT 1
# The minimum number of connections maintained by the connection pool.
org.sagebionetworks.pool.min.number.connections=2
# The maximum number of connections the connection pool will created
org.sagebionetworks.pool.max.number.connections=40

# The repository Apache HttpClient connection pool properties
org.sagebionetworks.httpclient.connectionpool.maxconnsperroute=20

# JDOPersistenceManager configuration properties.
javax.jdo.PersistenceManagerFactoryClass=org.datanucleus.jdo.JDOPersistenceManagerFactory
datanucleus.NontransactionalRead=true
datanucleus.NontransactionalWrite=true
javax.jdo.option.RetainValues=true
datanucleus.autoCreateSchema=true
datanucleus.validateConstraints=false
datanucleus.validateTables=false
datanucleus.transactionIsolation=read-committed

org.sagebionetworks.integration.test.username.one=devUser1@sagebase.org
org.sagebionetworks.integration.test.password.one=password
org.sagebionetworks.integration.test.email.one=devUser1@sagebase.org

org.sagebionetworks.integration.test.username.two=devUser2@sagebase.org
org.sagebionetworks.integration.test.password.two=password
org.sagebionetworks.integration.test.email.two=devUser2@sagebase.org

org.sagebionetworks.integration.test.username.three=integration.test@sagebase.org
org.sagebionetworks.integration.test.password.three=password
org.sagebionetworks.integration.test.email.three=integration.test@sagebase.org
org.sagebionetworks.integration.test.displayname.three=dev usr3

org.sagebionetworks.integration.test.username.rejecttermsofuse=rejecttermsofuse.integration.test@sagebase.org
org.sagebionetworks.integration.test.password.rejecttermsofuse=password
org.sagebionetworks.integration.test.email.rejecttermsofuse=rejecttermsofuse.integration.test@sagebase.org

org.sagebionetworks.integration.test.username.admin=devUserAdmin@sagebase.org
org.sagebionetworks.integration.test.password.admin=password

# Activity Logger
org.sagebionetworks.usage.metrics.logging.enabled=true

# Whether log sweeping should be enabled
org.sagebionetworks.logging.sweeper.enabled=true
org.sagebionetworks.logging.sweeper.delete.enabled=true
org.sagebionetworks.logging.sweeper.bucket=logs.sagebase.org

#AmazonWebServices CloudWatch Profiler 
org.sagebionetworks.cloud.watch.report.enabled=true

#AmazonWebServices CloudWatch Profiler's Trigger time in milliseconds
org.sagebionetworks.cloud.watch.trigger=60000

# The maximum number of threads used by the backup/restore daemon thread pool.
# Set this to an even number since two threads are used for each daemon.
org.sagebionetworks.backup.restore.thread.pool.maximum=10

# The maximum number of bytes allowed for a query result.  Currently set to 500 KB.
org.sagebionetworks.maximum.bytes.per.query.result=512000

# The maximum number entities returned in a single call
org.sagebionetworks.maximum.number.entities.returned.per.call=20

# This is a size limit on a single entity.
# Note: The number can never be decreased, so increase with care.
org.sagebionetworks.maximum.number.bytes.per.entity=1024000

# The maximum number of pixels used for a preview image h
org.sagebionetworks.preview.image.max.pixels=150

# Google email address for receiving Breast Cancer Challenge applications
org.sagebionetworks.bcc.approvalEmail=integration.test@sagebase.org

# a switch to disable the BCC Signup element of the Web UI
org.sagebionetworks.bcc.signup.enabled=true

org.sagebionetworks.bridge.spreadsheet.title=Bridge Spreadsheet

# the Consumer Key and Secret were set up using Admin tools for our Google hosted domain
org.sagebionetworks.bcc.googleapps.oauth.consumer.key=sagebase.org
# encrypted
org.sagebionetworks.bcc.googleapps.oauth.consumer.secret=eFRrk1D5T0XerPxoS3J0HcxyT6GOYPiEKHSGHsg0KUA=

# the following are generated using OAuthUtils.generateAccessToken()
# encrypted
org.sagebionetworks.bcc.googleapps.oauth.access.token=5nDka+sq5WHm7RZaLeKn5Nd2948HN/wFbYEDswD5Iep9/PFoLQ2tKTyO8xCVSN8W
#encrypted
org.sagebionetworks.bcc.googleapps.oauth.access.token.secret=M4guxd8yMXCzqapmIriChh0S+jhAHat/KHSGHsg0KUA=

# AWS Simple work flow settings:
org.sagebionetworks.swf.workflowExecutionRetentionPeriodInDays=1

# Portal LinkedIn Integration (secret in AWS stack specific configuration files)
org.sagebionetworks.portal.api.linkedin.key=0oq37ippxz8c
# Portal Get Satisfaction Integration
org.sagebionetworks.portal.api.getsatisfaction.key=z5e1lo36kro5

#encrypted
org.sagebionetworks.portal.api.linkedin.secret=YLUPEqtJfj90WngxCkqApih0hh7INClA
#encrypted
org.sagebionetworks.portal.api.getsatisfaction.secret=RmKXqRvGxYeN3yciA1P4TvCJueNgGApBWnwQGscV/a8odIYeyDQpQA==

org.sagebionetworks.repo.manager.jira.user.name=synapse-jira-service
org.sagebionetworks.repo.manager.jira.user.password=6PkUHq9yaYlIqlIZCUn0jmmR4IG0y8b+

# The percentage of the maximum memory that can be used for file transfer.
# Note: transfer% + preview% cannot exceed 90%
org.sagebionetworks.repo.manager.file.transfer.memory.percent.of.max=0.70
# The percentage of the maximum memory that can be used for preview generation.
# Note: transfer% + preview% cannot exceed 0.90 (90%)
org.sagebionetworks.repo.manager.file.preview.memory.percent.of.max=0.20
# This is the size of a single file transfer memory block used as a buffer. 
# Note: Due to S3 limitations on the minimum size of a single part of a multi-part upload
# this value cannot be less 5 MB.  It is currently set to 5 MB.
org.sagebionetworks.repo.manager.file.transfer.memory.buffer.bytes=5242880
# The maximum number of worker threads used to generate preview files.
org.sagebionetworks.file.preview.max.number.worker.threads=10
# The maximum number of entities that can be moved into the trash can at one time.
org.sagebionetworks.repo.manager.trash.max.trashable=500

# Should messages be published to the AWS topic?
org.sagebionetworks.repo.manage.shouldMessagesBePublishedToTopic=true

#--- EZID ---
org.sagebionetworks.ezid.username=apitest
org.sagebionetworks.ezid.password=vab8LnWEFVc=
org.sagebionetworks.ezid.url=https://n2t.net/ezid/
org.sagebionetworks.ezid.doi.prefix=doi:10.5072/FK2.
org.sagebionetworks.ezid.doi.target.url.prefix=https://www.synapse.org

# Backup batch size See PLFM-1896
org.sagebionetworks.repo.manager.migration.backup.batch.max=500

# This should match the Database max_allowed_packet value. See PLFM-1900
org.sagebionetworks.repo.model.dbo.migration.max.allowed.packet.byte = 1048576

# Configuration properties for the Semaphore gated runner.

# When a lock is acquired from the semaphore a timeout must be provided
# If the lock is not released before the timeout expires, the lock will be forfeit
# and another runner will be able to acquire the lock. This is currently set to
# 30 minutes to allow plenty of time for the works to finish their work.
# Setting this too low will undermine the gating.
org.sagebionetworks.semaphore.gated.lock.timeout.ms=1800000
# The maxiumn number of worker in the cluster that will process RDS index data
org.sagebionetworks.semaphore.gated.max.runners.rds=10
# The maxiumn number of worker in the cluster that will process search index data
org.sagebionetworks.semaphore.gated.max.runners.search=10
# The maxiumn number of worker in the cluster that will process file previews
org.sagebionetworks.semaphore.gated.max.runners.file.preview=10
# The maxiumn number of worker in the cluster that will process Dynamo index data
org.sagebionetworks.semaphore.gated.max.runners.dynamo.index=10
# The maxiumn number of worker in the cluster that will synchronize Dynamo with RDS
org.sagebionetworks.semaphore.gated.max.runners.dynamo.synchronize=1

#The maximum amount of time the multipart upload daemons are allowed to take before timing out.
org.sagebionetworks.repo.manager.file.multipart.upload.daemon.timeout.ms=60000
# The maximum number of threads that can be used for the mutipart upload daemons.
org.sagebionetworks.repo.manager.file.multipart.upload.daemon.main.max.threads=20
# The maximum number of threads that can be used for the mutipart upload daemons copy part sub-task.
org.sagebionetworks.repo.manager.file.multipart.upload.daemon.copy.part.max.threads=50

